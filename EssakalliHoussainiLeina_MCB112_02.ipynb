{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #importing Python modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-install kallisto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing Kallisto(which has been installed using the command line) - ! is used to run the command from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kallisto 0.46.0\r\n",
      "\r\n",
      "Usage: kallisto <CMD> [arguments] ..\r\n",
      "\r\n",
      "Where <CMD> can be one of:\r\n",
      "\r\n",
      "    index         Builds a kallisto index \r\n",
      "    quant         Runs the quantification algorithm \r\n",
      "    bus           Generate BUS files for single-cell data \r\n",
      "    pseudo        Runs the pseudoalignment step \r\n",
      "    merge         Merges several batch runs \r\n",
      "    h5dump        Converts HDF5-formatted results to plaintext\r\n",
      "    inspect       Inspects and gives information about an index\r\n",
      "    version       Prints version information\r\n",
      "    cite          Prints citation information\r\n",
      "\r\n",
      "Running kallisto <CMD> without arguments prints usage information for <CMD>\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kallisto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-reproduce Moriartyâ€™s result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index - generation using the arc.fasta example\n",
    "\n",
    "Generate a file called 'transcriptome.index' (index from the transcript using arc.fasta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[build] loading fasta file arc.fasta\r\n",
      "[build] k-mer length: 31\r\n",
      "[build] counting k-mers ... done.\r\n",
      "[build] building target de Bruijn graph ...  done \r\n",
      "[build] creating equivalence classes ...  done\r\n",
      "[build] target de Bruijn graph has 19 contigs and contains 10000 k-mers \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kallisto index -i transcriptome.idx arc.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quant is used to 'Runs the quantification algorithm' - Basically I want to generate the abundance for the reads the using the same input as Moriarty and expect the same output \n",
    "output is the name of the folder that we generate(name that I chose) \n",
    "--single is used because I'm trying to 'Quantify single-end reads'\n",
    "the reads are done from a 'cDNA fragment library of mean length 150bp , standard deviation 20bp'.\n",
    "\n",
    "Therefore I set the :\n",
    "'Estimated average fragment length' to 150 \n",
    "'Estimated standard deviation of fragment length 'to  20\n",
    "\n",
    "arc.fastq are the reads \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution is truncated gaussian with mean = 150, sd = 20\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 10\n",
      "[index] number of k-mers: 10,000\n",
      "[index] number of equivalence classes: 26\n",
      "[quant] running in single-end mode\n",
      "[quant] will process file 1: arc.fastq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 100,000 reads, 99,983 reads pseudoaligned\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 58 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kallisto quant -i transcriptome.idx -o output --single -l 150 -s 20 arc.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 100,000 reads, 99,983 reads pseudoaligned - this seems resonable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous line of code generated a folder 'Ouput' that contain 3 different documents :\n",
    "    _ abundance.h5\n",
    "    _ abundance.tsv\n",
    "    _run_info.json\n",
    "The next step is to read to  abundance.tsv file and check if the tpm for each arc correspond to the TPM obtaiend by Moriarty    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>target_id</td>\n",
       "      <td>length</td>\n",
       "      <td>eff_length</td>\n",
       "      <td>est_counts</td>\n",
       "      <td>tpm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arc1</td>\n",
       "      <td>4000</td>\n",
       "      <td>3851</td>\n",
       "      <td>2810.71</td>\n",
       "      <td>20570.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arc2</td>\n",
       "      <td>2000</td>\n",
       "      <td>1851</td>\n",
       "      <td>3640.85</td>\n",
       "      <td>55436.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arc3</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>28233.5</td>\n",
       "      <td>279105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arc4</td>\n",
       "      <td>4000</td>\n",
       "      <td>3851</td>\n",
       "      <td>10376.2</td>\n",
       "      <td>75939.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arc5</td>\n",
       "      <td>4000</td>\n",
       "      <td>3851</td>\n",
       "      <td>12912.9</td>\n",
       "      <td>94504.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arc6</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>1983.17</td>\n",
       "      <td>19604.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arc7</td>\n",
       "      <td>2000</td>\n",
       "      <td>1851</td>\n",
       "      <td>5442.49</td>\n",
       "      <td>82868.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arc8</td>\n",
       "      <td>2000</td>\n",
       "      <td>1851</td>\n",
       "      <td>5635.31</td>\n",
       "      <td>85804.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arc9</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>3082.56</td>\n",
       "      <td>30472.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arc10</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>25865.2</td>\n",
       "      <td>255693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2           3        4\n",
       "0   target_id  length  eff_length  est_counts      tpm\n",
       "1        Arc1    4000        3851     2810.71  20570.4\n",
       "2        Arc2    2000        1851     3640.85  55436.5\n",
       "3        Arc3    3000        2851     28233.5   279105\n",
       "4        Arc4    4000        3851     10376.2  75939.3\n",
       "5        Arc5    4000        3851     12912.9  94504.2\n",
       "6        Arc6    3000        2851     1983.17  19604.8\n",
       "7        Arc7    2000        1851     5442.49  82868.8\n",
       "8        Arc8    2000        1851     5635.31  85804.8\n",
       "9        Arc9    3000        2851     3082.56  30472.9\n",
       "10      Arc10    3000        2851     25865.2   255693"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data=pd.read_csv('output/abundance.tsv',header= None, index_col= False,sep=\"\\s+\")\n",
    "output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the results are close but not exactly the same as Moriaty we can conclude that the author rounded the obtained results \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcriptome example-2 first gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">ENST00000513300.5\r\n",
      "AGTGCCTTTGGCGCACTGAGGTGCACAGGGTCCCTTAGCCGGGCGCAGGGCGCGCAGCCCAGGCTGAGATCCGCGGCTTCCGTAGAAGTGAGCATGGCTGGGCAGCGAGTGCTTCTTCTAGTGGGCTTCCTTCTCCCTGGGGTCCTGCTCTCAGAGGCTGCCAAAATCCTGACAATATCTACAGTAGATTTTAAAAAGGAAGAAAAATCATATCAAGTTATCAGTTGGCTTGCACCTGAAGATCATCAAAGAGAATTTAAAAAGAGTTTTGATTTCTTTCTGGAAGAAACTTTAGGTGGCAGAGGAAAATTTGAAAACTTATTAAATGTTCTAGAATACTTGGCGTTGCAGTGCAGTCATTTTTTAAATAGAAAGGATATCATGGATTCCTTAAAGAATGAGAACTTCGACATGGTGATAGTTGAAACTTTTGACTACTGTCCTTTCCTGATTGCTGAGAAGCTTGGGAAGCCATTTGTGGCCATTCTTTCCACTTCATTCGGCTCTTTGGAATTTGGGCTACCAATCCCCTTGTCTTATGTTCCAGTATTCCGTTCCTTGCTGACTGATCACATGGACTTCTGGGGCCGAGTGAAGAATTTTCTGATGTTCTTTAGTTTCTGCAGGAGGCAACAGCACATGCAGTCTACATTTGACAACACCATCAAGGAACATTTCACAGAAGGCTCTAGGCCAGTTTTGTCTCATCTTCTACTGAAAGCAGAGTTGTGGTTCATTAACTCTGACTTTGCCTTTGATTTTGCTCGACCTCTGCTTCCCAACACTGTTTATGTTGGAGGCTTGATGGAAAAACCTATTAAACCAGTACCACAAGACTTGGAGAACTTCATTGCCAAGTTTGGGGACTCTGGTTTTGTCCTTGTGACCTTGGGCTCCATGGTGAACACCTGTCAGAATCCGGAAATCTTCAAGGAGATGAACAATGCCTTTGCTCACCTACCCCAAGGGGTGATATGGAAGTGTCAGTGTTCTCATTGGCCCAAAGATGTCCACCTGGCTGCAAATGTGAAAATTGTGGACTGGCTTCCTCAGAGTGACCTCCTGGCTCACCCAAGCATCCGTCTGTTTGTCACCCACGGCGGGCAGAATAGCATAATGGAGGCCATCCAGCATGGTGTGCCCATGGTGGGGATCCCTCTCTTTGGAGACCAGCCTGAAAACATGGTCCGAGTAGAAGCCAAAAAGTTTGGTGTTTCTATTCAGTTAAAGAAGCTCAAGGCAGAGACATTGGCTCTTAAGATGAAACAAATCATGGAAGACAAGAGATACAAGTCCGCGGCAGTGGCTGCCAGTGTCATCCTGCGCTCCCACCCGCTCAGCCCCACACAGCGGCTGGTGGGCTGGATTGACCACGTCCTCCAGACAGGGGGCGCGACGCACCTCAAGCCCTATGTCTTTCAGCAGCCCTGGCATGAGCAGTACCTGCTCGACGTTTTTGTGTTTCTGCTGGGGCTCACTCTGGGGACTCTATGGCTTTGTGGGAAGCTGCTGGGCATGGCTGTCTGGTGGCTGCGTGGGGCCAGAAAGGTGAAGGAGACATAAGGCCAGGTGCAGCCTTGGCGGGGTCTGTTTGGTGGGCGATGTCACCATTTCTAGGGAGCTTCCCACTAGTTCTGGCAGCCCCATTCTCTAGTCCTTCTAGTTATCTCCTGTTTTCTTGAAGAACAGGAAAAATGGCCAAAAATCATCCTTTCCACTTGCTAATTTTGCTACAAATTCATCCTTACTAGCTCCTGCCTGCTAGCAGAATTCTTTCCAGTCCTCTTGTCCTCCTTTGTTTGCCATCAGCAAGGGCTATGCTGTGATTCTGTCTCTGAGTGACTTGGACCACTGACCCTCAGATTTCCAGCCTTAAAATCCACCTTCCTTCTCATGCGCCTCTCCGAATCACACCCTGACTCTT\r\n",
      ">ENST00000282507.7\r\n",
      "CCCACAGCCAGTGCCTTTGGCGCACTGAGGTGCACAGGGTCCCTTAGCCGGGCGCAGGGCGCGCAGCCCAGGCTGAGATCCGCGGCTTCCGTAGAAGTGAGCATGGCTGGGCAGCGAGTGCTTCTTCTAGTGGGCTTCCTTCTCCCTGGGGTCCTGCTCTCAGAGGCTGCCAAAATCCTGACAATATCTACAGTAGGTGGAAGCCATTATCTACTGATGGACCGGGTTTCTCAGATTCTTCAAGATCACGGTCATAATGTCACCATGCTTAACCACAAAAGAGGTCCTTTTATGCCAGATTTTAAAAAGGAAGAAAAATCATATCAAGTTATCAGTTGGCTTGCACCTGAAGATCATCAAAGAGAATTTAAAAAGAGTTTTGATTTCTTTCTGGAAGAAACTTTAGGTGGCAGAGGAAAATTTGAAAACTTATTAAATGTTCTAGAATACTTGGCGTTGCAGTGCAGTCATTTTTTAAATAGAAAGGATATCATGGATTCCTTAAAGAATGAGAACTTCGACATGGTGATAGTTGAAACTTTTGACTACTGTCCTTTCCTGATTGCTGAGAAGCTTGGGAAGCCATTTGTGGCCATTCTTTCCACTTCATTCGGCTCTTTGGAATTTGGGCTACCAATCCCCTTGTCTTATGTTCCAGTATTCCGTTCCTTGCTGACTGATCACATGGACTTCTGGGGCCGAGTGAAGAATTTTCTGATGTTCTTTAGTTTCTGCAGGAGGCAACAGCACATGCAGTCTACATTTGACAACACCATCAAGGAACATTTCACAGAAGGCTCTAGGCCAGTTTTGTCTCATCTTCTACTGAAAGCAGAGTTGTGGTTCATTAACTCTGACTTTGCCTTTGATTTTGCTCGACCTCTGCTTCCCAACACTGTTTATGTTGGAGGCTTGATGGAAAAACCTATTAAACCAGTACCACAAGACTTGGAGAACTTCATTGCCAAGTTTGGGGACTCTGGTTTTGTCCTTGTGACCTTGGGCTCCATGGTGAACACCTGTCAGAATCCGGAAATCTTCAAGGAGATGAACAATGCCTTTGCTCACCTACCCCAAGGGGTGATATGGAAGTGTCAGTGTTCTCATTGGCCCAAAGATGTCCACCTGGCTGCAAATGTGAAAATTGTGGACTGGCTTCCTCAGAGTGACCTCCTGGCTCACCCAAGCATCCGTCTGTTTGTCACCCACGGCGGGCAGAATAGCATAATGGAGGCCATCCAGCATGGTGTGCCCATGGTGGGGATCCCTCTCTTTGGAGACCAGCCTGAAAACATGGTCCGAGTAGAAGCCAAAAAGTTTGGTGTTTCTATTCAGTTAAAGAAGCTCAAGGCAGAGACATTGGCTCTTAAGATGAAACAAATCATGGAAGACAAGAGATACAAGTCCGCGGCAGTGGCTGCCAGTGTCATCCTGCGCTCCCACCCGCTCAGCCCCACACAGCGGCTGGTGGGCTGGATTGACCACGTCCTCCAGACAGGGGGCGCGACGCACCTCAAGCCCTATGTCTTTCAGCAGCCCTGGCATGAGCAGTACCTGCTCGACGTTTTTGTGTTTCTGCTGGGGCTCACTCTGGGGACTCTATGGCTTTGTGGGAAGCTGCTGGGCATGGCTGTCTGGTGGCTGCGTGGGGCCAGAAAGGTGAAGGAGACATAAGGCCAGGTGCAGCCTTGGCGGGGTCTGTTTGGTGGGCGATGTCACCATTTCTAGGGAGCTTCCCACTAGTTCTGGCAGCCCCATTCTCTAGTCCTTCTAGTTATCTCCTGTTTTCTTGAAGAACAGGAAAAATGGCCAAAAATCATCCTTTCCACTTGCTAATTTTGCTACAAATTCATCCTTACTAGCTCCTGCCTGCTAGCAGAATTCTTTCCAGTCCTCTTGTCCTCCTTTGTTTGCCATCAGCAAGGGCTATGCTGTGATTCTGTCTCTGAGTGACTTGGACCACTGACCCTCAGATTTCCAGCCTTAAAATCCACCTTCCTTCTCATGCGCCTCTCCGAATCACACCCTGACTCTTCCAGCCTCCATGTCCAGACCTAGTCAGCCTCTCTCACTCCTGCCCCTACTATCTATCATGGAATAACATCCAAGAAAGACACCTTGCATATTCTTTCAGTTTCTGTTTTGTTCTCCCACATATTCTCTTCAATGCTCAGGAAGCCTGCCCTGTGCTTGAGAGTTCAGGGCCGGACACAGGCTCACAGGTCTCCACATTGGGTCCCTGTCTCTGGTGCCCACAGTGAGCTCCTTCTTGGCTGAGCAGGCTTGGAGACTGTAGGTTTCCAGATTTCCTGAAAAATAAAAGTTTACAGCGTTATCTCTCCCCAACCTCACTAA\r\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c test_copy/transcripts.fasta.gz | head -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 first genes from transcripts.fasta, I used head -4 because I want to 2 genes and each gene is represented in 2 line \n",
    "-One for the name \n",
    "-One for the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three reads from reads_1.fastq, I used head -12 because I want 3 reads and each read is represented in 4 lines :\n",
    "\n",
    "-one for the name\n",
    "\n",
    "-one for the sequence \n",
    "\n",
    "-one with a + \n",
    "\n",
    "-one with quality values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@1:NM_014620:16:182\r\n",
      "GTTCCGAGCGCTCCGCAGAACAGTCCTCCCTGTAAGAGCCTAACCATTGC\r\n",
      "+\r\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\r\n",
      "@2:NM_014620:1094:172\r\n",
      "ATGAAAAAAATTCACGTTAGCACGGTGAACCCCAATTATAACGGAGGGGA\r\n",
      "+\r\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\r\n",
      "@3:NM_022658:294:172\r\n",
      "TGTACGGGCCCGGCGGCTCGGCGCCCGGCTTCCAGCACGCTTCGCACCAC\r\n",
      "+\r\n",
      "IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\r\n",
      "gunzip: error writing to output: Broken pipe\r\n",
      "gunzip: test_copy/reads_1.fastq.gz: uncompress failed\r\n"
     ]
    }
   ],
   "source": [
    "! gunzip -c test_copy/reads_1.fastq.gz | head -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs 3 reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 simulate an Arc transcriptome and RNA-seq reads\n",
    "\n",
    "I breaked this into 3 subparts\n",
    "- produce the FASTA file \n",
    "- generate 1 read and write it into a FASTQ file\n",
    "- produce the FASTQ file (with 100 000 reads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 produce the FASTA file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.A- Create a first dictionary dict_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dictionary called dict_segment\n",
    "each key is a letter from A to J(corresponding to the 10 different segement) and each value is the corresponding 1000 nt sequence(randomly generated)\n",
    "Each nucleotide has an equal probablity to be 'A'-'C'-'T'or 'G' therefore proba=[0.25,0.25,0.25,0.25]\n",
    "The way I implemented it : for loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_segment= {} #initialisation \n",
    "proba=[0.25,0.25,0.25,0.25]\n",
    "sequence_length=1000\n",
    "letter=['A','B','C','D','E','F','G','H','I','J']\n",
    "for i in letter: \n",
    "    string_name=(i)\n",
    "    string_sequence=''.join(np.random.choice(list('ATCG'),sequence_length,p=proba))\n",
    "    dict_segment[(string_name)] = string_sequence #set the key(letter of the segement) and the value(sequence of that segment)\n",
    "np.random.seed(1)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.B- Create a second dictionary for the transcripts \n",
    "Each transcripts(arc1..arc10) is composed of a combination of different segements.\n",
    "3.1.C- Create a fasta file for the trancripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generation of .fasta file for trancripts: \n",
    "The way I did it was to use the values of the dic_segment to extract the corresponding sequence of each transcript(knowing that is it always a combination of different segements(A..J).\n",
    "\n",
    "In the same for loop I also created a dictionary called fragment_segment\n",
    "Each key is the trancript name(Arc1 for exemple) and each value is the corresponding sequence(for Arc1 it is the sequence composed of the fragements'A','B','C'&'D')\n",
    "\n",
    "\n",
    "Please note that i choose to not use modular - why ? because i think using the dictionary composed of segements I can generate transcripts with  more flexibility ( theorically we can generate a transcrpit composed of segment A,C&J for exemple) that would be harder to implement with modular.\n",
    "\n",
    "I commented the Arc_1 part bellow and all the the other transcripted have been written in the file in a similar way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=open('transcripts.fasta', 'w') #we will store \n",
    "fragment_segment= {} #initialisation \n",
    "Arc_1= ('A','B','C','D') #composed of the segments 'A','B','C','D' - will be used to loop over \n",
    "string_name=('>'+'Arc1') #format for trancripts name in a fasta file\n",
    "a.write(string_name) #write >Arc1 to the file \n",
    "a.write('\\n') #go to the line \n",
    "temp_list = [] #temporary list, will be used in the for loop\n",
    "for i in Arc_1:  #loop over all the segments that are composing Arc_1\n",
    "    a.write(dict_segment[i]) #write sequence segement i in the transcripts.fasta file (i will take the value of A,B,C,D in this order)\n",
    "    temp_list.append(dict_segment[i]) #append to the temporary list (stores temporarly the sequence)\n",
    "    fragment_segment['arc1']= ''.join(temp_list) #set the key to be Arc1& \n",
    "    #assigns the value to be the sequence \n",
    "    #''.join is used of to remove the ','\n",
    "a.write('\\n')    #go to the line before the next transcript \n",
    "\n",
    "\n",
    "\n",
    "Arc_2= ('B','C')\n",
    "string_name=('>'+'Arc2')\n",
    "a.write(string_name)\n",
    "a.write('\\n')  \n",
    "sequence_Arc_2=() #initialize \n",
    "temp_list = [] #reinitialize the list \n",
    "for i in Arc_2:\n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc2']= ''.join(temp_list)\n",
    "a.write('\\n')     \n",
    "    \n",
    "Arc_3= ('C','D','E')\n",
    "sequence_Arc_3=()\n",
    "string_name=('>'+'Arc3')\n",
    "a.write(string_name)\n",
    "a.write('\\n') \n",
    "temp_list = [] #reinitialize the list \n",
    "for i in Arc_3: \n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc3']= ''.join(temp_list)\n",
    "a.write('\\n') \n",
    "    \n",
    "Arc_4= ('D','E','F','G')\n",
    "sequence_Arc_4=()\n",
    "string_name=('>'+'Arc4')\n",
    "a.write(string_name)\n",
    "a.write('\\n') \n",
    "temp_list = []\n",
    "for i in Arc_4:  \n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc4']= ''.join(temp_list)\n",
    "a.write('\\n')        \n",
    "    \n",
    "Arc_5= ('E','F','G','H')\n",
    "sequence_Arc_5=()\n",
    "string_name=('>'+'Arc5')\n",
    "a.write(string_name)\n",
    "a.write('\\n') \n",
    "temp_list = []\n",
    "for i in Arc_5:  \n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc5']= ''.join(temp_list)\n",
    "a.write('\\n')    \n",
    "    \n",
    "Arc_6= ('F','G','H')\n",
    "sequence_Arc_6=()\n",
    "string_name=('>'+'Arc6')\n",
    "a.write(string_name)\n",
    "a.write('\\n') \n",
    "temp_list = []\n",
    "for i in Arc_6: \n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc6']= ''.join(temp_list)\n",
    "a.write('\\n') \n",
    "\n",
    "Arc_7= ('G','H')\n",
    "sequence_Arc_7=()\n",
    "string_name=('>'+'Arc7')\n",
    "a.write(string_name)\n",
    "a.write('\\n') \n",
    "temp_list = []\n",
    "for i in Arc_7: \n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc7']= ''.join(temp_list)\n",
    "a.write('\\n') \n",
    "\n",
    "    \n",
    "Arc_8= ('H','I')\n",
    "sequence_Arc_8=()\n",
    "string_name=('>'+'Arc8')\n",
    "a.write(string_name)\n",
    "a.write('\\n')\n",
    "temp_list = []\n",
    "for i in Arc_8:\n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc8']= ''.join(temp_list)\n",
    "a.write('\\n')\n",
    "  \n",
    "    \n",
    "Arc_9= ('I','J','A')\n",
    "sequence_Arc_9=()\n",
    "string_name=('>'+'Arc9')\n",
    "a.write(string_name)\n",
    "a.write('\\n')\n",
    "temp_list = []\n",
    "for i in Arc_9:\n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc9']= ''.join(temp_list)\n",
    "a.write('\\n')\n",
    "     \n",
    "    \n",
    "Arc_10= ('J','A','B')\n",
    "sequence_Arc_10=()\n",
    "string_name=('>'+'Arc10')\n",
    "a.write(string_name)\n",
    "a.write('\\n')\n",
    "temp_list = []\n",
    "for i in Arc_10:\n",
    "    a.write(dict_segment[i])\n",
    "    temp_list.append(dict_segment[i])\n",
    "    fragment_segment['arc10']= ''.join(temp_list)\n",
    "a.write('\\n')\n",
    "a.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we generated the FASTA file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 generate 1 read and write it into a FASTQ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the final FASTQ file will be composed of 100 000 reads- but I chose to first generate 1 single read and then create a function that include a for loop to generate 100 000k reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 3.2.1 select a sample a transcript i according to its nucleotide abundance Î½i\n",
    "\n",
    "pick one of 10 transcripts based on abundances\n",
    "picked_transcript is the randomly picked trancrips( on of the 10 arcs)\n",
    "picked_sequence is its corresponding sequence(accessed using the dictionary created in 3.1B \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum of the nomralised abundances is 1.0\n",
      "the selected transcript is arc4\n",
      "its corresponding sequence is   : TGGAAATAGCTCTGACTACAGACCGCAGAAATGTCAATTTGTTCCTCGGGATTTTAGCATCCCTCCGTATTCTGTCCACCACCCCTCGACCTAAGCGGGGGGGCAAATTGGGGCGTGCCCATATACTCAGTCTTGTCGACCTAGCCAGCGTAATTGATCAGGCAATTTGACGATGCTACAACTTCGGGTACGGAAACCTACTCACACGCGAACATGAATGTCAATTCTAAATTAACAGGTTCAACGCGACACTTATTTGGCGAGTTTGTGAACGCGAGCGGGCTGTACTACGCTTCACAGCAGGTTCGCAGAATACAAAACGCGGGCTCCATAAGCTGAAAACGCAGGCTGCATCTACGTTTCCCGCTGACCGCTTCACGTATATTGAGTAACTAGATTTTAAAGATATAAAGGTCTCAGCCCGTTAGATGAGTCTAATCAGAAAGAGTCAGACTGCCGACTAGCAAGTCACGTGCGTTATTGTTATGTCGGATAATACGTCGAGGGTATATGACAGGCATCTCTTCACATGGGCTAATCCCACTTCACCAGTTTTTTACGGAGCACTAATCTTAGTCACAAAAGCTGCAAAATTACGAATATTCAAAACACGCGACCTTAAATCGGCGAGGTATAGTTCAGGGAGGCAATATGTCACCAGGTGAGTGGGGGTGACACAACAAGGACAGAGCTCTAGTTAGGCCGTGTCGAAGTGTCTGCCCGAAATAATCTATGACACCACGGGTATAATAAAAGGGCTTTCCATCTACTCGCCAACTCGAAGAGCCAACCGGAACCCTCTCTGGTGGAGATCCCGGCTTGGTATCAAGGGGAGCAATCGGCCATTCAAGGCGAAGCTGTAGGAATTGGAATTCTTCACGACCCGGTAAAGGCGGTAGAATAGCAGCGTACTGAACAAGAGAGATCAGTACGAAGGAATTCAGCCTGTCGAAGTTGCCAGCGTGCAAGGATCGGGCGTGTTTTCCACAGGGCCTGGTGGGTTCCTGTTATGTAGCTAAGTTCCTGTCGGGTAATTGGACTCGCTGGGGTACGCAAAAGTCTTAGAAAGAATGTGTACTTAGAGAGTGTGTTAGACCCTGTAAAAGTATCAACCGTCGATGTGAATACTTACAGGGAATTTAGACGGATGCAGTCATTCGCAGATCAGATAATACATAAGACGTAGGGTTGTAGGAGCACATCCCCGCATATGTGAACACAACCTTTAGTTCCCCCCGAGTGGTGTCAGACTGGCGGGACCACGGTGTGTCCTTGACCTCGCTAGTTTACTATTGACACCGGAGCGTCCGTGACCAAGGAGACTTTTGTGAACGACAACCCTTGCGTTTGGAAGTGTCAAGCAAGGCTAGATAGAATACGTGCCAGGCCGTTTAAATGTTTCATTACGAGAGCCGGCACACCAAATGCTATCTCAACAGGAGCCTTGCCACGAGGCCCCAACGCTGGTATCTGAAACGAAGCTGCAGACCCGGTTATTCGTCCTGAGACGAAGCACACATGTTGCCCCTCTGTAAGTGGAAACACTACCAAAGACAGTAAAAGACTCCATCCGCTAAGCACCATGATAACCTCGGGTGCGGCTAGCGATAGTGATTACGAAGCTCTCAAGTTCGTGAAGAGGCGACGCATATTATGAAAAGTCTTGCGGTGCCATAAAACCGCGGTCTGGGCGCGCACGCTAGCTGAGTCGAAGAATTTCCTGGTTCAACGCTCGCTATGGAGCGGTATTATGGAGCGCGACCAGGGTGTCGGGGTGCCTTTCAAGCGGTACGTGGCACAAATTAACTACACTTGAAGGACAGCATTCCAGATTTGTTCCGGGCCGCACGGACATCTGGCATACACGGTAAAAACGTACTCAAGACGGCCTCACCGCCAGTAGTTGGGGTGCCCTTGGGGCAGCTCCTCCGCAACGAAGGAGGCTTCACTGGGGATGAAATGGAAACGGCTGGTAATGTACTCTGGTCCGTTCGCCAGAGGGTCGGGGGGGCACCCTCAGGAACTAGCACATGATCGCTCGGAAGTAGGGATGCGCGTTCGTAGCGAACCCATCTTCGATTAGTATATGCAATCTAACTGCCAAGCACGAAACGGTATAGAATTGCCTGTGTTGTTGCCTATCGGTCGATTCACTAAGGCGTCTGGGGATCGCCCAGAACGCGACATTTTAATGTATTCGGTGATAGATGATGGTCGTCAATTCCTCATTCCCCCATAGTTCATGCCTGAAGGATGGAATCTATGGTGGCATCGCGCCTCCTCTGTCTTTTAACTGTGCTAGATACGGTGGGAGCAACTAGAGTATCCCCATCAGTTAACCTAATGTCGTTTTCTCAACGCGCCTCAATGGCCACTAGTACTCTTTCGCGTACACGGAGCAATTCCATCTTTCCAGCGGGTGCCACCGTGTCGAAAACGAACTAGCCTTCAGCATCTTGGCAGCCTCACCTCTACGTTAGTCATAGAATGCTTAGTGTTATCTGGCTTTCGGGACTTAAACTAAGGTACGAGAGAGTTAAGGTGAAATCGTAAGTACATGTTATAGTGTGAGTTACGATTTCTTTATGTAAATACTTCCGCGTTGATTTGCCCGGAACCGCATACGGTTCGTAGTTAGGTGCTTATATTATAACGACCATGTGCACCGTTGGCGAACGCCTTTCAGCCTCCTTCTGAATCTCATATGTGGCCGTTCCATACATGCCGATTTGGGAAACCTATTCCGAGGACCCTGGTAGATTCTTCGGAACCCCGGCTGCGCGCAGGACAGGGACCCGCTCGCACCCGGAACACGATGGTCGCTTACGTCCAATCCATCATACATTCTTGGGTCGGTTCCTCTGACGCCATTCTGTTTCGCATCGATCTGTTATGAACGTTCCATCCAATGATGCGCCAGATTCGTATGAGAGAGGCCTCGCAGCCGCGGCTACTGTGGCTCGTTTAGTCTATTTCCATTAGAAATCGGGTCTTACAATTATATCTCTAAATTGGTTGGAATCCTGTGGTTGGATCGCTCGGTGTGCGGAGCTTGCTAATGACTTCAAAGACACCCATACGGTATTGGATCGCAATAAAAAAAGATCGGGCTAACGCATACCGTATGTGCCTATCTGATGAGGTCCAACTGGGTCTAACAGGAGTGTCTGCATCGAGCGGTCATTCGTCAGAACTCCAACTACCGTGAGTCTACCTATCACGCACGAACGAAATTGGTTTTGGTCAGTCTTAGCTACTTATCACCACTGTAAGGGAGGGAATTGGGAGAGAAGACAATGCTACCCGGAGCCATATATTTTCAGACCTCCTTGGGACAGTTCTGCGACAAGAAAGCTCGGTCACCTCGGGACAGATGATGACTTGGACGTACAGGGTCGAAAGATCACTGGCCTCTGGGTACCGGTATCCATTAGCAAGCGTTTCTGGATGAAGAGCACACAACGGAAGGTAACCAGCCTAGTAGCCGCTTTCGGTCGGTCGAAGGCGTTCATACAGGACTTAAATACGTGCCAGTGAGTGGATTAAAGAGAGGTAGAAGGCGCATATTGAACCACGTCCGGAACGGCACTTCGGCTAGCGTGAATCCGATCAGTCACTTTACGAGTCTCTCATCTCACCTCCAGTGGCCGGCACTGAAAGAGCGATAAAGCTAGCTGATTATAACGCAACACCACACCATGGATCGGGCCTGGTCTATTGCGCGAAACCCTCGGGAGCTCAAAAGAACTTGTGGTAGCATATGAACTATCCTTCGAAGAATATAATCCTTCCCCGATTTGTGCCCCAGGGCGACGCACGAACTCCTGAACGCTGATTCATAAGACATGAAGTGCCGTCAGAAAATCGAACCACGTAGCTCCTTGGCAATTAACTCTGCTCTTCTAATACGGAAGTCTCTCCGGTCCAAAGGTCTCGGCTTTTATAAGGTTTATGTTCGCTTCGCGCCACTGTTTAGAGATTTCGCGTATGGTGATTCGTC\n"
     ]
    }
   ],
   "source": [
    "abundances=[0.008,0.039,0.291,0.112,0.127,0.008,0.059,0.060,0.022,0.273] #extracted from the table\n",
    "len(abundances) #check that we have 10 abundances (1 for each trancript)\n",
    "sum(abundances) #check - the probablity dont add to 1 - need to normalize \n",
    "normalised_factor = sum(abundances)\n",
    "abundances_normalised_transcripts = [x / normalised_factor for x in abundances]\n",
    "print('the sum of the nomralised abundances is',sum(abundances_normalised_transcripts))  #check - this now sum up to 1.\n",
    "arcs=['arc1','arc2','arc3','arc4','arc5','arc6','arc7','arc8','arc9','arc10'] #possible randoms\"picks\"\n",
    "#np.random.seed(1) #used for reproducibility\n",
    "picked_transcript=np.random.choice(arcs,p=abundances_normalised_transcripts) #peaks one of the arc using the abundances\n",
    "\n",
    "print('the selected transcript is',picked_transcript) #test\n",
    "picked_sequence=fragment_segment[picked_transcript] #sequence of the transcript selected out of the 10 transcripts\n",
    "print('its corresponding sequence is   :',picked_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 3.2.2 pick a random fragment length at least as long as one read from a truncated Gaussian of mean length 150, standard deviation 20 (truncated because: no shorter than a read, and no longer than the whole transcript), \n",
    "please note that for this part I mainly used the code provided in the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the random length is  133 long\n"
     ]
    }
   ],
   "source": [
    "L_picked_sequence=len(picked_sequence)\n",
    "mean_frag=150\n",
    "sd_frag=20\n",
    "len_R=75\n",
    "#np.random.seed(2)\n",
    "while True:\n",
    "    fraglen = int(np.random.normal(mean_frag, sd_frag))\n",
    "    if fraglen >= len_R: break\n",
    "if fraglen > L_picked_sequence: fraglen = L_picked_sequence\n",
    "print('the random length is ', fraglen,'long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2.3 pick a random fragment of that length from transcript i;\n",
    "\n",
    "upper limit of what we can use as my index to select the random fragement \n",
    "I will refer to Random fragment length as RFL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the random fragement selected out of the sequence transcript is CCAGCGTAATTGATCAGGCAATTTGACGATGCTACAACTTCGGGTACGGAAACCTACTCACACGCGAACATGAATGTCAATTCTAAATTAACAGGTTCAACGCGACACTTATTTGGCGAGTTTGTGAACGCGA\n"
     ]
    }
   ],
   "source": [
    "upper_limit_index=len(picked_sequence)-fraglen+1\n",
    "#np.random.seed(3)\n",
    "start_index=np.random.randint(0,upper_limit_index)#generate an int that can take the maximum value of upper_limit_index\n",
    "end_index=start_index+fraglen\n",
    "#print(start_index)\n",
    "#print(end_index)\n",
    "\n",
    "RFL_picked_sequence=picked_sequence[start_index:end_index]\n",
    "print('the random fragement selected out of the sequence transcript is', RFL_picked_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2.4 pick a 75nt read by choosing a random orientation, taking the read from one of the two ends of the fragment. If on the top strand, your read is the first 75nt of the fragment. If on the bottom strand, your read is the last 75nt of the fragment, reverse complemented.\n",
    "\n",
    "the orientation is either forward or reverse the 2 senario happen with equal probablity - \n",
    "so the function np.random.choice(oritentations_possibilities,p=proba_orientation) will return forward or reverse with probablity 1/2\n",
    "\n",
    "if the orientation is forward we just have to read the first 75 nt - stored into a variable called selected_75nt\n",
    "\n",
    "if the is reverse we have to do several step \n",
    "1)select the last 75nt \n",
    "2)reverse the order (ex if the sequence is ATCG the reverse ordered one will be GCTA(like reading from right to left)\n",
    "3)translate the reversed sequence - A replaced by T , and G replaced by C (and visversa)\n",
    "T=str.maketrans('ATCG','TAGC') is set the translation that I want to make \n",
    ".translate(T) is used to apply the translation to the sequence of interest\n",
    "\n",
    "\n",
    "the output of 3) is stored into a variable called selected_75nt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "CCAGCGTAATTGATCAGGCAATTTGACGATGCTACAACTTCGGGTACGGAAACCTACTCACACGCGAACATGAAT\n"
     ]
    }
   ],
   "source": [
    "proba_orientation=[0.5,0.5]\n",
    "oritentations_possibilities=['forward','reverse']\n",
    "T=str.maketrans('ATCG','TAGC')\n",
    "oritentation=np.random.choice(oritentations_possibilities,p=proba_orientation)\n",
    "print(oritentation) #check\n",
    "if oritentation=='forward': \n",
    "    selected_75nt = RFL_picked_sequence[0:75] #first 75 nt\n",
    "    print(selected_75nt)\n",
    "elif oritentation=='reverse': \n",
    "    reverse_orientation_75nt=(RFL_picked_sequence[-75:]) #last 75 nt\n",
    "    print('the end read seq-normal order')\n",
    "    print(reverse_orientation_75nt) #check\n",
    "    reverse_orientation_75ntreverse=(list(reversed(reverse_orientation_75nt))) #reverse order \n",
    "    print('the end read seq-reverse order')\n",
    "    print(reverse_orientation_75ntreverse) #check\n",
    "    \n",
    "    selected_75nt=''.join(reverse_orientation_75ntreverse).translate(T) #'',jointo input a string instead of list\n",
    "    print('the reversed translated read seq')\n",
    "    print(selected_75nt)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2.5 generate a simulated read sequence by adding simulated base calling errors to the 75nt read: given base\n",
    "calling accuracy , at each base, with probability(1-Alpha).\n",
    "\n",
    "the way I implemented it - using a for loop to go through all the nucleotides that compose selected_75nt\n",
    "for each one, i called an np.random.choice that will set read_i to error with a probablity of 1-0.999 ( so 0.001)\n",
    "so in most of the cases nothing will happen but when the read_i is error we have to switch this nucleotide base with one of the 3 others \n",
    "Say for exemple that the nucleotide for which I need to generate an error is A this nucleotide can only be replaced by T,C or G but NOT A. And this nucleotide has to be replaced with T C G with equal probablities(so 1/3 for each of the 3 bases?\n",
    "How did i implemented that - first i created set_possibility which contains all four nucleotides and I substracted from it the nucelotide that need to be replaced (set(set_possibility) - set(i))\n",
    "\n",
    "and then passed a np.random.choice with arguments replacement_possibilities as outcome and p=[1/3,1/3,1/3].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCAGCGTAATTGATCAGGCAATTTGACGATGCTACAACTTCGGGTACGGAAACCTACTCACACGCGAACATGAAT\n"
     ]
    }
   ],
   "source": [
    "set_possibility=['A','T','C','G']\n",
    "alpha=0.999\n",
    "\n",
    "for i in selected_75nt :\n",
    "    read_i=np.random.choice([i,'error'],p=[alpha,1-alpha])\n",
    "    if read_i == 'error': #if read_i is error it means that the base has to be changed \n",
    "        replacement_possibilities= set(set_possibility) - set(i) #it can only be changed by a base that is different from the one it is \n",
    "        i=np.random.choice(list(replacement_possibilities),p=[1/3,1/3,1/3])#all other bases have equal probablities to be selected\n",
    "print(selected_75nt)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I generated 1 random read I want to write it into a file using the fastq format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b=open('test.fastq', 'w') #creation of a file called test.fatq\n",
    "read_name='read1' \n",
    "b.write('@:'+read_name)\n",
    "b.write('\\n')\n",
    "b.write(selected_75nt) \n",
    "b.write('\\n')\n",
    "b.write('+')\n",
    "b.write('\\n')\n",
    "quality_values= \"I\" * len(selected_75nt) #prints 75 I's\n",
    "b.write(quality_values)\n",
    "b.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 produce the FASTQ file (with 100 000 reads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this part I basically used the code that I generated in the 3.2 part and just added a four loop \n",
    "I re-wrote the constants used (that have been defined progressively in 3.2), just to be more organised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants- dont change for all the 1K reads \n",
    "abundances=[0.008,0.039,0.291,0.112,0.127,0.008,0.059,0.060,0.022,0.273] \n",
    "normalised_factor = sum(abundances)\n",
    "abundances_normalised_transcripts= [x / normalised_factor for x in abundances]\n",
    "arcs=['arc1','arc2','arc3','arc4','arc5','arc6','arc7','arc8','arc9','arc10']\n",
    "mean_frag=150\n",
    "sd_frag=20\n",
    "len_R=75\n",
    "set_possibility=['A','T','C','G']\n",
    "alpha=0.999\n",
    "proba_orientation=[0.5,0.5]\n",
    "oritentations_possibilities=['forward','reverse']\n",
    "T=str.maketrans('ATCG','TAGC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function called read_function takes 4 arguments \n",
    "- nb_of_reads - the number of reads we want to generate\n",
    "- file_fastq - file in which we want to write the reads generated\n",
    "- dict_transcripts -that correspond to different possible transcripts (fragment_segment)\n",
    "- abundances_normalised- and the abundances associated with each one of the trancripts \n",
    "\n",
    "\n",
    "The final goal of this fucntion is to write the FASTQ file generating the reads using a list of possible transcripts and the abundances associated with each of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_function(nb_of_reads,file_fastq,dict_transcripts,abundances_normalised):\n",
    "    for read_rep in range(0,nb_of_reads): \n",
    "        \n",
    "        np.random.seed(read_rep) #make sure to generate a new sequence everytime, use read_rep as an argument of np.random.seed()- but used for reproducibility\n",
    "        picked_transcript=np.random.choice(arcs,p=abundances_normalised)\n",
    "        picked_sequence=dict_transcripts[picked_transcript] #sequence of the transcript selected out of the 10 transcripts\n",
    "    #depending on the abundances\n",
    "        L_picked_sequence=len(picked_sequence)\n",
    "    \n",
    "        while True:\n",
    "            fraglen = int(np.random.normal(mean_frag, sd_frag))\n",
    "            if fraglen >= len_R: break\n",
    "        \n",
    "    \n",
    "   \n",
    "    \n",
    "        upper_limit_index=len(picked_sequence)-fraglen\n",
    "        start_index=np.random.randint(0,upper_limit_index)#generate an int that can take the maximum value of upper_limit_index\n",
    "        end_index=start_index+fraglen    \n",
    "        RFL_picked_sequence=picked_sequence[start_index:end_index]#random fragment of that length from transcript i\n",
    "    \n",
    "    \n",
    "        oritentation=np.random.choice(oritentations_possibilities,p=proba_orientation)\n",
    "    \n",
    "        if oritentation=='forward': \n",
    "             selected_75nt = RFL_picked_sequence[0:75] #first 75 nt\n",
    "        elif oritentation=='reverse': \n",
    "            reverse_orientation_75nt=(RFL_picked_sequence[-75:]) #last 75 nt\n",
    "            reverse_orientation_75ntreverse=(list(reversed(reverse_orientation_75nt)))\n",
    "            selected_75nt=''.join(reverse_orientation_75ntreverse).translate(T)\n",
    "  \n",
    "      \n",
    "        for i in selected_75nt :\n",
    "            read_i=np.random.choice([i,'error'],p=[alpha,1-alpha])\n",
    "            if read_i == 'error': #if read_i is error it means that the base has to be changed \n",
    "                replacement_possibilities= set(set_possibility) - set(i) #it can only be changed by a base that is different from the one it is \n",
    "            \n",
    "                i=np.random.choice(list(replacement_possibilities),p=[1/3,1/3,1/3])\n",
    "            \n",
    "    #writing in the file  \n",
    "            \n",
    "        read_name='@:read'+str(read_rep+1)  #so no 0 \n",
    "        file_fastq.write(read_name)\n",
    "        file_fastq.write('\\n')\n",
    "        file_fastq.write(selected_75nt)\n",
    "        file_fastq.write('\\n')\n",
    "        file_fastq.write('+')\n",
    "        file_fastq.write('\\n')\n",
    "        quality_values= \"I\" * 75\n",
    "        file_fastq.write(quality_values)\n",
    "        file_fastq.write('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling the function read_function- the 100 000 reads will be outputed in the readL.fastq file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c=open('readL.fastq', 'w') #creation of a file \n",
    "read_function(nb_of_reads=100000,file_fastq=c,dict_transcripts=fragment_segment,abundances_normalised=abundances_normalised_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 test kallisto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[build] loading fasta file transcripts.fasta\r\n",
      "[build] k-mer length: 31\r\n",
      "[build] counting k-mers ... done.\r\n",
      "[build] building target de Bruijn graph ...  done \r\n",
      "[build] creating equivalence classes ...  done\r\n",
      "[build] target de Bruijn graph has 19 contigs and contains 10000 k-mers \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kallisto index -i transcriptome_2.idx transcripts.fasta #generating the index using the transcripts.fasta file I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution is truncated gaussian with mean = 150, sd = 20\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 10\n",
      "[index] number of k-mers: 10,000\n",
      "[index] number of equivalence classes: 26\n",
      "[quant] running in single-end mode\n",
      "[quant] will process file 1: readL.fastq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 99,992 reads, 99,992 reads pseudoaligned\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 57 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kallisto quant -i transcriptome_2.idx -o output_2 --single -l 150 -s 20 readL.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>moriarty_results</th>\n",
       "      <th>expected_TPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>target_id</td>\n",
       "      <td>length</td>\n",
       "      <td>eff_length</td>\n",
       "      <td>est_counts</td>\n",
       "      <td>tpm</td>\n",
       "      <td>moriarty</td>\n",
       "      <td>expected_tpm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arc1</td>\n",
       "      <td>4000</td>\n",
       "      <td>3851</td>\n",
       "      <td>2939.2</td>\n",
       "      <td>21500.9</td>\n",
       "      <td>21000</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arc2</td>\n",
       "      <td>2000</td>\n",
       "      <td>1851</td>\n",
       "      <td>3666.61</td>\n",
       "      <td>55803.4</td>\n",
       "      <td>55000</td>\n",
       "      <td>58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arc3</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>28313.6</td>\n",
       "      <td>279769</td>\n",
       "      <td>280000</td>\n",
       "      <td>290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arc4</td>\n",
       "      <td>4000</td>\n",
       "      <td>3851</td>\n",
       "      <td>10521.8</td>\n",
       "      <td>76969.4</td>\n",
       "      <td>76000</td>\n",
       "      <td>83000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arc5</td>\n",
       "      <td>4000</td>\n",
       "      <td>3851</td>\n",
       "      <td>12700</td>\n",
       "      <td>92903.4</td>\n",
       "      <td>95000</td>\n",
       "      <td>94000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arc6</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>1823.37</td>\n",
       "      <td>18016.9</td>\n",
       "      <td>20000</td>\n",
       "      <td>7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arc7</td>\n",
       "      <td>2000</td>\n",
       "      <td>1851</td>\n",
       "      <td>5468.69</td>\n",
       "      <td>83229.7</td>\n",
       "      <td>83000</td>\n",
       "      <td>87000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arc8</td>\n",
       "      <td>2000</td>\n",
       "      <td>1851</td>\n",
       "      <td>5681.56</td>\n",
       "      <td>86469.5</td>\n",
       "      <td>86000</td>\n",
       "      <td>88000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arc9</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>3163.21</td>\n",
       "      <td>31255.9</td>\n",
       "      <td>30000</td>\n",
       "      <td>22000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arc10</td>\n",
       "      <td>3000</td>\n",
       "      <td>2851</td>\n",
       "      <td>25714</td>\n",
       "      <td>254082</td>\n",
       "      <td>260000</td>\n",
       "      <td>270000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2           3        4 moriarty_results  \\\n",
       "0   target_id  length  eff_length  est_counts      tpm         moriarty   \n",
       "1        Arc1    4000        3851      2939.2  21500.9            21000   \n",
       "2        Arc2    2000        1851     3666.61  55803.4            55000   \n",
       "3        Arc3    3000        2851     28313.6   279769           280000   \n",
       "4        Arc4    4000        3851     10521.8  76969.4            76000   \n",
       "5        Arc5    4000        3851       12700  92903.4            95000   \n",
       "6        Arc6    3000        2851     1823.37  18016.9            20000   \n",
       "7        Arc7    2000        1851     5468.69  83229.7            83000   \n",
       "8        Arc8    2000        1851     5681.56  86469.5            86000   \n",
       "9        Arc9    3000        2851     3163.21  31255.9            30000   \n",
       "10      Arc10    3000        2851       25714   254082           260000   \n",
       "\n",
       "    expected_TPM  \n",
       "0   expected_tpm  \n",
       "1           6000  \n",
       "2          58000  \n",
       "3         290000  \n",
       "4          83000  \n",
       "5          94000  \n",
       "6           7800  \n",
       "7          87000  \n",
       "8          88000  \n",
       "9          22000  \n",
       "10        270000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_TPM=['expected_tpm',6000,58000,290000,83000,94000,7800,87000,88000,22000,270000]\n",
    "moriarty_results=['moriarty',21000,55000,280000,76000,95000,20000,83000,86000,30000,260000]\n",
    "output2_data=pd.read_csv('output_2/abundance.tsv',header= None, index_col= False,sep=\"\\s+\")\n",
    "output2_data['moriarty_results']=moriarty_results\n",
    "output2_data['expected_TPM']=expected_TPM\n",
    "\n",
    "output2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Debug Kalisto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table bellow showed the the tpm obtained are very close to the results obtained by Moriarty but not close to the expected ones. \n",
    "I believe that the issue is that kallisto dont take as a hypothesis the overlap. \n",
    "To test I will first create a new fasta file composed of transcripts wtih no overlap \n",
    "arc1-segmentA \n",
    "ARC2-segmentB \n",
    "etc \n",
    "very similar to the process of what i used before to generate a transcript fasta file and dictionary corresponding to transcripts  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of another fasta and dictionatry called fragment_segment_debug\n",
    "#each transcript is composed of 1 segment only(so no possible overlap)\n",
    "d=open('debug.fasta', 'w') #creation of a file \n",
    "proba=[0.25,0.25,0.25,0.25]\n",
    "sequence_length=1000\n",
    "fragment_segment_debug= {} #initialisation \n",
    "\n",
    "\n",
    "Arc_1_debug= ('A')\n",
    "string_name=('>'+'Arc1')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['A']) \n",
    "fragment_segment_debug['arc1']=dict_segment['A']\n",
    "d.write('\\n')   \n",
    "\n",
    "Arc_2_debug= ('B')\n",
    "string_name=('>'+'Arc2')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['B']) \n",
    "fragment_segment_debug['arc2']=dict_segment['B']\n",
    "d.write('\\n')\n",
    "  \n",
    "Arc_3_debug= ('C')\n",
    "string_name=('>'+'Arc3')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['C']) \n",
    "fragment_segment_debug['arc3']=dict_segment['C']\n",
    "d.write('\\n')\n",
    "\n",
    "\n",
    "Arc_4_debug= ('D')\n",
    "string_name=('>'+'Arc4')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['D']) \n",
    "fragment_segment_debug['arc4']=dict_segment['D']\n",
    "d.write('\\n')\n",
    "        \n",
    "\n",
    "Arc_5_debug= ('D')\n",
    "string_name=('>'+'Arc5')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['E']) \n",
    "fragment_segment_debug['arc5']=dict_segment['E']\n",
    "d.write('\\n')\n",
    "        \n",
    "    \n",
    "Arc_6_debug= ('F')\n",
    "string_name=('>'+'Arc6')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['F']) \n",
    "fragment_segment_debug['arc6']=dict_segment['F']\n",
    "d.write('\\n')    \n",
    "        \n",
    "    \n",
    "Arc_7_debug= ('G')\n",
    "string_name=('>'+'Arc7')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['G']) \n",
    "fragment_segment_debug['arc7']=dict_segment['G']\n",
    "d.write('\\n')    \n",
    "            \n",
    "    \n",
    "    \n",
    "Arc_8_debug= ('H')\n",
    "string_name=('>'+'Arc8')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['H']) \n",
    "fragment_segment_debug['arc8']=dict_segment['H']\n",
    "d.write('\\n')    \n",
    "               \n",
    "    \n",
    "Arc_9_debug= ('I')\n",
    "string_name=('>'+'Arc9')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['I']) \n",
    "fragment_segment_debug['arc9']=dict_segment['I']\n",
    "d.write('\\n')    \n",
    "                \n",
    "    \n",
    "Arc_10_debug= ('J')\n",
    "string_name=('>'+'Arc10')\n",
    "d.write(string_name)\n",
    "d.write('\\n')\n",
    "d.write(dict_segment['J']) \n",
    "fragment_segment_debug['arc10']=dict_segment['J']\n",
    "d.write('\\n')    \n",
    "                      \n",
    "d.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to test my result I chose to set the abundances of the 5 first transcripts to be the same and half of the abundances for the 5 last transcripts \n",
    "If the 'bug' from kallisto is due to the overlap, we will obtain similar tpm for the arcs from 1 to 5 which is half of the tpm for the arcs 6 to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abundances_debug=[1,1,1,1,1,2,2,2,2,2] \n",
    "normalised_factor_debug = sum(abundances_debug)\n",
    "abundances_normalised_transcripts_debug= [x / normalised_factor_debug for x in abundances_debug]\n",
    "debug_fastq_file=open('read_debug.fastq', 'w') #creation of a file \n",
    "#CALL the function created above using with different arugments  \n",
    "read_function(nb_of_reads=100000,file_fastq=debug_fastq_file,dict_transcripts=fragment_segment_debug,abundances_normalised=abundances_normalised_transcripts_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[build] loading fasta file debug.fasta\r\n",
      "[build] k-mer length: 31\r\n",
      "[build] counting k-mers ... done.\r\n",
      "[build] building target de Bruijn graph ...  done \r\n",
      "[build] creating equivalence classes ...  done\r\n",
      "[build] target de Bruijn graph has 10 contigs and contains 9700 k-mers \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kallisto index -i transcriptome_3.idx debug.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[quant] fragment length distribution is truncated gaussian with mean = 150, sd = 20\n",
      "[index] k-mer length: 31\n",
      "[index] number of targets: 10\n",
      "[index] number of k-mers: 9,700\n",
      "[index] number of equivalence classes: 10\n",
      "[quant] running in single-end mode\n",
      "[quant] will process file 1: read_debug.fastq\n",
      "[quant] finding pseudoalignments for the reads ... done\n",
      "[quant] processed 99,992 reads, 99,065 reads pseudoaligned\n",
      "[   em] quantifying the abundances ... done\n",
      "[   em] the Expectation-Maximization algorithm ran for 52 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kallisto quant -i transcriptome_3.idx -o output_3 --single -l 150 -s 20 read_debug.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>target_id</td>\n",
       "      <td>length</td>\n",
       "      <td>eff_length</td>\n",
       "      <td>est_counts</td>\n",
       "      <td>tpm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arc1</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>6770</td>\n",
       "      <td>68339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arc2</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>6547</td>\n",
       "      <td>66087.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arc3</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>6534</td>\n",
       "      <td>65956.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arc4</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>6520</td>\n",
       "      <td>65815.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arc5</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>6546</td>\n",
       "      <td>66077.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arc6</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>13256</td>\n",
       "      <td>133811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arc7</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>13169</td>\n",
       "      <td>132933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arc8</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>13358</td>\n",
       "      <td>134841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arc9</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>13207</td>\n",
       "      <td>133317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arc10</td>\n",
       "      <td>1000</td>\n",
       "      <td>851</td>\n",
       "      <td>13158</td>\n",
       "      <td>132822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2           3        4\n",
       "0   target_id  length  eff_length  est_counts      tpm\n",
       "1        Arc1    1000         851        6770    68339\n",
       "2        Arc2    1000         851        6547  66087.9\n",
       "3        Arc3    1000         851        6534  65956.7\n",
       "4        Arc4    1000         851        6520  65815.4\n",
       "5        Arc5    1000         851        6546  66077.8\n",
       "6        Arc6    1000         851       13256   133811\n",
       "7        Arc7    1000         851       13169   132933\n",
       "8        Arc8    1000         851       13358   134841\n",
       "9        Arc9    1000         851       13207   133317\n",
       "10      Arc10    1000         851       13158   132822"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3_data=pd.read_csv('output_3/abundance.tsv',header= None, index_col= False,sep=\"\\s+\")\n",
    "output3_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the final results obtained is what I would expect - this confirms that the reason Kalisto is messing up with the arcs is because of the overlaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
